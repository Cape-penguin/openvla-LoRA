{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6843d1b",
   "metadata": {},
   "source": [
    "# Implements OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56802369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n",
    "# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\", \n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Grab image input & format prompt\n",
    "image: Image.Image = get_from_camera(...)\n",
    "prompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "# Execute...\n",
    "robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75990f",
   "metadata": {},
   "source": [
    "# Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5558cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 예시: 가장 유명한 데이터셋 중 하나인 'fractal20220825_data' (RT-1 데이터)\n",
    "ds = tfds.load('fractal20220825_data', split='train', data_dir='YOUR_DATA_PATH')\n",
    "for episode in ds.take(1):\n",
    "    for step in episode['steps'].take(1):\n",
    "        print(step['observation']['image'])  # 이미지 데이터 확인\n",
    "        print(step['action'])               # 액션 값 확인\n",
    "        print(step['observation']['natural_language_instruction']) # 언어 명령 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a240523",
   "metadata": {},
   "source": [
    "# RLDS (Robot Learning Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b61673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 데이터셋 경로 설정 (tfrecord 파일이 들어있는 '상위 폴더' 경로를 넣으세요)\n",
    "DATA_PATH = \"/workspace/openvla-LoRA/data/bridge_dataset-train.tfrecord-00000-of-01024\" \n",
    "\n",
    "# 2. RLDS 데이터셋 로드\n",
    "# 해당 경로에 metadata가 포함되어 있어야 작동합니다.\n",
    "builder = tfds.builder_from_directory(DATA_PATH)\n",
    "ds = builder.as_dataset(split='train')\n",
    "\n",
    "# 3. 데이터 확인하기 (첫 번째 에피소드의 첫 번째 스텝)\n",
    "for episode in ds.take(1):\n",
    "    steps = list(episode['steps'])\n",
    "    first_step = steps[0]\n",
    "    \n",
    "    # 이미지 데이터 추출\n",
    "    image = first_step['observation']['image'].numpy()\n",
    "    instruction = first_step['observation']['natural_language_instruction'].numpy().decode('utf-8')\n",
    "    action = first_step['action'].numpy()\n",
    "    \n",
    "    print(f\"Task: {instruction}\")\n",
    "    print(f\"Action (Joint velocities/Pose): {action}\")\n",
    "    \n",
    "    # 이미지 시각화\n",
    "    display(Image.fromarray(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
