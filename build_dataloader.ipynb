{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144189e8",
   "metadata": {},
   "source": [
    "# Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6d2489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>category</th>\n",
       "      <th>instruction</th>\n",
       "      <th>anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/scripted_raw/2022-12-08_pnp_rigid_objec...</td>\n",
       "      <td>2022-12-08_pnp_rigid_objects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/scripted_raw/2022-12-08_pnp_rigid_objec...</td>\n",
       "      <td>2022-12-08_pnp_rigid_objects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/scripted_raw/2022-12-08_pnp_rigid_objec...</td>\n",
       "      <td>2022-12-08_pnp_rigid_objects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/scripted_raw/2022-12-08_pnp_rigid_objec...</td>\n",
       "      <td>2022-12-08_pnp_rigid_objects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/scripted_raw/2022-12-08_pnp_rigid_objec...</td>\n",
       "      <td>2022-12-08_pnp_rigid_objects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  ./data/scripted_raw/2022-12-08_pnp_rigid_objec...   \n",
       "1  ./data/scripted_raw/2022-12-08_pnp_rigid_objec...   \n",
       "2  ./data/scripted_raw/2022-12-08_pnp_rigid_objec...   \n",
       "3  ./data/scripted_raw/2022-12-08_pnp_rigid_objec...   \n",
       "4  ./data/scripted_raw/2022-12-08_pnp_rigid_objec...   \n",
       "\n",
       "                       category  instruction   anno  \n",
       "0  2022-12-08_pnp_rigid_objects          NaN  False  \n",
       "1  2022-12-08_pnp_rigid_objects          NaN  False  \n",
       "2  2022-12-08_pnp_rigid_objects          NaN  False  \n",
       "3  2022-12-08_pnp_rigid_objects          NaN  False  \n",
       "4  2022-12-08_pnp_rigid_objects          NaN  False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./instruction.csv', )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34678c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(df.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388c9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if not os.path.exists(row['path']):\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a84006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "TRAJ_PATH = \"./data/scripted_raw/sweep_12-03/2022-12-04_14-56-20/raw/traj_group0/traj0\"\n",
    "INST = \"In order to pick up the can, the robot should\"\n",
    "\n",
    "image = Image.open(os.path.join(TRAJ_PATH, f\"images0/im_0.jpg\")).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5c52c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/openvla/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "inputs = processor(INST, image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f106e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12]), torch.Size([1, 12]), torch.Size([1, 6, 224, 224]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e05ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(TRAJ_PATH, \"policy_out.pkl\"), \"rb\") as f:\n",
    "    raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6fb791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actions': array([-0.02172307,  0.04577763, -0.00310825, -0.00611765, -0.02040617,\n",
       "         -0.19583636,  0.99728119])},\n",
       " {'actions': array([-0.01741809,  0.05146871, -0.00563831, -0.01201281, -0.04797513,\n",
       "         -0.19332831,  0.98917208])},\n",
       " {'actions': array([-0.01222756,  0.04357178, -0.00229091,  0.00568858, -0.04464969,\n",
       "         -0.14133196,  1.        ])},\n",
       " {'actions': array([-0.02752886,  0.0356793 ,  0.01062104,  0.01323281, -0.01837164,\n",
       "         -0.12258763,  1.        ])},\n",
       " {'actions': array([-0.00523159,  0.0306144 ,  0.00577714,  0.02011717, -0.00497684,\n",
       "         -0.09514776,  1.        ])},\n",
       " {'actions': array([-8.97468404e-03,  1.92724973e-02, -4.68816489e-04,  1.03511757e-02,\n",
       "          1.21088095e-02, -7.63940667e-02,  9.97315548e-01])},\n",
       " {'actions': array([-1.03006529e-02,  1.77663997e-02,  1.90194032e-04,  1.11419430e-02,\n",
       "          8.67873884e-03, -5.00243020e-02,  9.97342255e-01])},\n",
       " {'actions': array([-9.05113337e-03,  7.17515094e-03,  3.28887577e-04,  3.42727377e-03,\n",
       "          1.78802442e-02, -1.95689832e-02,  1.00000000e+00])},\n",
       " {'actions': array([-0.00470854,  0.0044096 ,  0.00438938, -0.00289651,  0.01569852,\n",
       "         -0.01239187,  1.        ])},\n",
       " {'actions': array([-1.02544681e-02,  7.90109162e-03,  2.10658265e-03, -7.76115736e-04,\n",
       "          2.58674490e-02, -5.86775865e-03,  1.00000000e+00])},\n",
       " {'actions': array([-0.00866151,  0.00462036, -0.03058218,  0.00184971,  0.0083162 ,\n",
       "         -0.0038719 ,  0.00397009])},\n",
       " {'actions': array([-0.00427339, -0.0063068 , -0.0252746 ,  0.00873004,  0.00962661,\n",
       "         -0.00935533, -0.01152137])},\n",
       " {'actions': array([-0.0023439 ,  0.00403247, -0.0272736 ,  0.00302253, -0.00530174,\n",
       "          0.00371541,  0.00344106])},\n",
       " {'actions': array([-0.00122535, -0.00039451, -0.02049911,  0.01308631, -0.00416508,\n",
       "         -0.00727509, -0.0003593 ])},\n",
       " {'actions': array([-0.00338282,  0.00249415, -0.00787116, -0.00079775, -0.00118388,\n",
       "          0.00134837, -0.00127822])},\n",
       " {'actions': array([ 0.00021386, -0.00412967, -0.008724  ,  0.00921618, -0.01262937,\n",
       "          0.01522656,  0.0002296 ])},\n",
       " {'actions': array([ 0.0069984 ,  0.00328232,  0.00208565, -0.00628886, -0.00682957,\n",
       "          0.00211932,  0.00043587])},\n",
       " {'actions': array([ 0.0010516 , -0.00189834, -0.00678691, -0.00253882, -0.00917304,\n",
       "         -0.00711562,  0.00424594])},\n",
       " {'actions': array([-0.00718467, -0.00497717,  0.00312195, -0.00822571, -0.01222444,\n",
       "         -0.00365203, -0.00203177])},\n",
       " {'actions': array([ 0.01937605, -0.02231878,  0.00892335, -0.01194544, -0.01165439,\n",
       "          0.18501591, -0.00318527])},\n",
       " {'actions': array([ 0.01920609, -0.01043198, -0.00352973, -0.00681724,  0.00284444,\n",
       "          0.17871042,  0.0044313 ])},\n",
       " {'actions': array([ 0.01924064, -0.01523996,  0.00131761, -0.00236896,  0.00328634,\n",
       "          0.16424214, -0.00188435])},\n",
       " {'actions': array([-0.00497969, -0.00210421, -0.01001072, -0.00819751,  0.00279209,\n",
       "         -0.0586619 ,  0.00125488])},\n",
       " {'actions': array([ 0.02181257, -0.00972233,  0.00118389, -0.00720025,  0.00534602,\n",
       "          0.10572453,  0.00172632])},\n",
       " {'actions': array([ 0.01286382, -0.01323238,  0.00421875,  0.00716511, -0.00204321,\n",
       "          0.08960537, -0.00239524])},\n",
       " {'actions': array([ 0.00529982, -0.01243305, -0.00307027, -0.00553092, -0.00363142,\n",
       "          0.08994356, -0.00958729])},\n",
       " {'actions': array([ 0.00534455, -0.00816607,  0.00145572, -0.0035981 , -0.00211936,\n",
       "          0.06210395, -0.00748286])},\n",
       " {'actions': array([ 0.00072563, -0.00466262, -0.00011029, -0.00533232, -0.00456223,\n",
       "          0.04933201,  0.00034767])},\n",
       " {'actions': array([-0.00148566, -0.00214222, -0.00269785, -0.00634548,  0.01319404,\n",
       "          0.02281726, -0.0005469 ])}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0]['actions']\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17972c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-12 14:41:04.122584: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 14:41:04.147860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-12 14:41:04.147890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-12 14:41:04.148576: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-12 14:41:04.153356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-12 14:41:04.657848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/user/miniconda3/envs/openvla/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/openvla/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvla/openvla-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 1. 모델 로드\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m vla \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForVision2Seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenvla/openvla-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     92\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,                         \u001b[38;5;66;03m# Rank\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# 3. LoRA 모델로 변환\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:558\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/modeling_utils.py:3544\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   3543\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m-> 3544\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/modeling_utils.py:1454\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1454\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_sdpa(\n\u001b[1;32m   1464\u001b[0m         config,\n\u001b[1;32m   1465\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/modeling_utils.py:1546\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1543\u001b[0m install_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1548\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class Traj:\n",
    "    def __init__(self, traj_dir):\n",
    "        self.path = traj_dir\n",
    "        self.img_dir = os.path.join(traj_dir, \"images0\")\n",
    "        self.img_len = len(os.listdir(os.path.join(traj_dir, \"images0\")))\n",
    "        with open(os.path.join(traj_dir, \"policy_out.pkl\"), \"rb\") as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        self.actions = [d['actions'] for d in raw_data]\n",
    "\n",
    "        assert len(self.actions) == (self.img_len - 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def getitem(self, idx):\n",
    "        return Image.open(os.path.join(self.img_dir, f\"im_{idx}.jpg\")).convert(\"RGB\"), self.actions[idx]\n",
    "    \n",
    "    def getitems(self):\n",
    "        ims = []\n",
    "        for idx in range(self.img_len - 1):\n",
    "            im, _ = self.getitem(idx)\n",
    "            ims.append(im)\n",
    "        return ims, self.actions\n",
    "\n",
    "class BridgeDatasetV2(Dataset):\n",
    "    def __init__(self, traj_dirs, instructions, processor, vla_config):\n",
    "        self.processor = processor\n",
    "        self.instructions = instructions\n",
    "        self.vla_config = vla_config\n",
    "        self.traj_dirs = traj_dirs\n",
    "        self.trajs, self.total_len = self.load_trajs()\n",
    "\n",
    "        self.ims = []\n",
    "        self.actions = []\n",
    "\n",
    "        for traj in self.trajs:\n",
    "            I, A = traj.getitems()\n",
    "            self.ims.extend(I)\n",
    "            self.actions.extend(A)\n",
    "\n",
    "    def load_trajs(self, ):\n",
    "        trajs = []\n",
    "        cnt = 0\n",
    "        for traj_dir in self.traj_dirs:\n",
    "            obj = Traj(traj_dir)\n",
    "            trajs.append(obj)\n",
    "            cnt += len(obj)\n",
    "        return trajs, cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.ims[idx]\n",
    "        inputs = self.processor(\"\", image, return_tensors=\"pt\")\n",
    "\n",
    "        raw_action = np.array(self.actions[idx], dtype=np.float32)\n",
    "        bin_indices = np.clip((raw_action + 1.0) / 2.0 * 255, 0, 255).astype(np.int32)\n",
    "\n",
    "        action_token_ids = torch.tensor(bin_indices + 31000, dtype=torch.long)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "        labels[-7:] = action_token_ids\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "\n",
    "# 1. 모델 로드\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,                         # Rank\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # Attention 레이어 타겟\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 3. LoRA 모델로 변환\n",
    "vla = get_peft_model(vla, config)\n",
    "print(f\"trainable parameters: {vla.print_trainable_parameters()}\")\n",
    "\n",
    "df = pd.read_csv('./instruction.csv', )\n",
    "\n",
    "train_dataset = BridgeDatasetV2(\n",
    "    traj_dirs=df['path'].to_list(),\n",
    "    instructions=df['instruction'].to_list(),\n",
    "    processor=processor,\n",
    "    vla_config=vla.config\n",
    ")\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"DataLoader Length: {len(train_dataset)}\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"Input IDs shape: {batch['input_ids'].shape}\")     # [BS, Seq_Len]\n",
    "print(f\"Pixel Values shape: {batch['pixel_values'].shape}\") # [BS, 3, 224, 224]\n",
    "print(f\"Labels shape: {batch['labels'].shape}\")           # [BS, 7] (7 action tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
